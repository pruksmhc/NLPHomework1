{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "# split train data into actual train and validation set. \n",
    "DIRLINK = \"aclImdb\"\n",
    "def random_shuffle(threads, labels):\n",
    "\t\"\"\"\n",
    "\tthis function randomly shuffles the data and labels\n",
    "\t\"\"\"\n",
    "\tthreads = np.array(threads)\n",
    "\tlabels = np.array(labels)\n",
    "\ts = np.arange(labels.shape[0])\n",
    "\tnp.random.shuffle(s)\n",
    "\tthreads = threads[s]\n",
    "\tlabels = labels[s]\n",
    "\treturn threads, labels\n",
    "\n",
    "def get_test_train(traintest):\n",
    "    positive = [open(DIRLINK+\"/\"+traintest+\"/pos/\"+file).read() for file in os.listdir(DIRLINK+\"/\"+traintest+\"/pos/\")]\n",
    "    negative = [open(DIRLINK+\"/\"+traintest+\"/neg/\"+file).read() for file in os.listdir(DIRLINK+\"/\"+traintest+\"/neg/\")]\n",
    "    labels = [1]* len(positive)\n",
    "    data = positive\n",
    "    data.extend(negative)\n",
    "    labels.extend([0]*len(negative))\n",
    "    # shuffle the dataset\n",
    "    return data, labels \n",
    "\n",
    "#train_data, train_labels = get_test_train(\"train\")\n",
    "#test_data, test_labels = get_test_train(\"test\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Split train data into actual train and validation sets\\nLEN_TRAIN = len(train_data)\\nLEN_TEST = len(test_data)\\ntrain_datadf = train_data[:int(round(LEN_TRAIN*0.8))]\\ntrain_labeldf = train_labels[:int(round(LEN_TRAIN*0.8))]\\nval_data = train_data[int(round(LEN_TRAIN*0.8)):]\\nval_labels = train_labels[int(round(LEN_TRAIN*0.8)):]\\n\\nprint (\"Train dataset size is {}\".format(len(train_datadf)))\\nprint (\"Val dataset size is {}\".format(len(val_data)))\\nprint (\"Test dataset size is {}\".format(len(test_data)))\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Split train data into actual train and validation sets\n",
    "LEN_TRAIN = len(train_data)\n",
    "LEN_TEST = len(test_data)\n",
    "train_datadf = train_data[:int(round(LEN_TRAIN*0.8))]\n",
    "train_labeldf = train_labels[:int(round(LEN_TRAIN*0.8))]\n",
    "val_data = train_data[int(round(LEN_TRAIN*0.8)):]\n",
    "val_labels = train_labels[int(round(LEN_TRAIN*0.8)):]\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_datadf)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Random sample from train dataset\\nimport random\\nprint (train_data[random.randint(0, len(train_data) - 1)])\\n#!pip install spacy\\n#!python -m spacy download en_core_web_sm\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Random sample from train dataset\n",
    "import random\n",
    "print (train_data[random.randint(0, len(train_data) - 1)])\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/yadapruksachatkun/miniconda3/envs/nlpclass/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /Users/yadapruksachatkun/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[31mmkl-random 1.0.1 requires cython, which is not installed.\u001b[0m\n",
      "\u001b[31mmkl-fft 1.0.4 requires cython, which is not installed.\u001b[0m\n",
      "\u001b[31mtensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9af9de18071b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mwordnet_lemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m# Packages which can be lazily imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/nltk/stem/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misri\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mISRIStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrslp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRSLPStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuffix_replace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_replace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/nltk/corpus/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyCorpusLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m abc = LazyCorpusLoader(\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/nltk/corpus/reader/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenseval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mieer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msinica_treebank\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbracket_parse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindian\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import spacy\n",
    "import string\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "!pip install nltk\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [str(token).lower() for token in parsed if (str(token) not in punctuations)]\n",
    "\n",
    "def get_ngram(sample, n_gram):\n",
    "    sample = sample.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(sample)-n_gram+1):\n",
    "        output.append(sample[i:i+n_gram])\n",
    "    final = [' '.join(x) for x in output]\n",
    "    return final\n",
    "\n",
    "def strip_html(sample): \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sample)\n",
    "    return cleantext \n",
    "\n",
    "def tokenize_dataset(dataset, name, n_gram):\n",
    "    token_dataset = {i:[] for i in range(1, n_gram + 1)}\n",
    "    all_datasets = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        sample = str(sample)\n",
    "        #sample = strip_html(sample)\n",
    "        for i in range(1, n_gram+1):\n",
    "            sample_after = get_ngram(sample, i)\n",
    "            tokens = lower_case_remove_punc(sample_after)\n",
    "            token_dataset[i].append(tokens)\n",
    "    for i in range(1, n_gram+1):\n",
    "        pickle.dump(token_dataset[i],open(name+\"tokens_dataset\"+str(i)+\".pkl\", \"wb\") ) \n",
    "        all_datasets.append(token_dataset[i])\n",
    "    return all_datasets\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(dataset):\n",
    "    new_dataset = []\n",
    "    for review in dataset:\n",
    "        new_review = []\n",
    "        for word in review:\n",
    "            new_review.append(wordnet_lemmatizer.lemmatize(word))\n",
    "        new_dataset.append(\"\".join(new_review))\n",
    "    return new_dataset\n",
    "\n",
    "#tokenized_train = tokenize_dataset(lemmatize(train_datadf), \"trainlemma\", 2)\n",
    "#tokenized_val = tokenize_dataset(lemmatize(val_data) , \"vallemma\", 2)\n",
    "#tokenized_test = tokenize_dataset(lemmatize(test_data), \"testlemma\", 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "unigram_train = pickle.load(open(\"trainlemmatokens_dataset2.pkl\", \"rb\"))\n",
    "unigram_val = pickle.load(open(\"vallemmatokens_dataset2.pkl\", \"rb\"))\n",
    "unigram_test = pickle.load(open(\"testlemmatokens_dataset2.pkl\", \"rb\"))\n",
    "train_labeldf = pickle.load(open(\"traintokens_labels.pkl\", \"rb\") )\n",
    "val_labels = pickle.load(open(\"valtokens_labels.pkl\", \"rb\") )\n",
    "test_labels = pickle.load(open(\"testtokens_labels.pkl\", \"rb\") )\n",
    "\n",
    "pickle.dump(train_labeldf,open(\"traintokens_labels.pkl\", \"wb\") ) \n",
    "pickle.dump(val_labels,open(\"valtokens_labels.pkl\", \"wb\") ) \n",
    "pickle.dump(test_labels,open(\"testtokens_labels.pkl\", \"wb\") ) \n",
    "# these are the labels\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "//I'm doing this to be able to move cells up or down easily. \n",
    "IPython.keyboard_manager.command_shortcuts.add_shortcut('Ctrl-k', {\n",
    "    help : 'move up selected cells',\n",
    "    help_index : 'jupyter-notebook:move-selection-up',\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.move_selection_up();\n",
    "        return false;\n",
    "    }}\n",
    ");\n",
    "\n",
    "IPython.keyboard_manager.command_shortcuts.add_shortcut('Ctrl-j', {\n",
    "    help : 'move down selected cells',\n",
    "    help_index : 'jupyter-notebook:move-selection-down',\n",
    "    handler :  function (event) {\n",
    "        IPython.notebook.move_selection_down();\n",
    "        return false;\n",
    "    }}\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# find the 1000 most common words and subwords. \n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "# flatten the toeknized train to become list of all the tokens\n",
    "#unigram_flattened_traintokens = [j for sub in unigram_train for j in sub]\n",
    "\n",
    "#zoken2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#token2id, id2token = build_vocab(unigram_flattened_traintokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\"\"\"\n",
    "train_data_indices = token2index_dataset(unigram_train)\n",
    "val_data_indices = token2index_dataset(unigram_val)\n",
    "test_data_indices = token2index_dataset(unigram_test)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(len(train_labeldf))\n",
    "print(len(val_labels))\n",
    "print(len(test_labels))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def moviegroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "\"\"\"\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,10)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        # view basically reshapes it, so this averages it out. \n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "#model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def test_model_routine(train_loader, val_loader, model, criterion, optimizer, num_epochs, learning_rate):\n",
    "    total_acc = []\n",
    "    for epoch in range(num_epochs):\n",
    "        acc = []\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model) \n",
    "                train_acc = test_model(train_loader, model)\n",
    "                acc.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Train Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc, train_acc))\n",
    "        #scheduler.step(loss)\n",
    "        print(\"Average accuracy is\"+ str(np.mean(acc)))\n",
    "        total_acc.append(np.mean(acc))\n",
    "    print(\"total average accuarcies\")\n",
    "    print(total_acc)\n",
    "#test_model_routine(train_loader, val_loader, model, criterion, optimizer, num_epochs, learning_rate)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NOW, LET'S USE ONLY BIGRAMS NON-LEMMATIZED\n",
    "bigram_train = pickle.load(open(\"traintokens_dataset1.pkl\", \"rb\"))\n",
    "bigram_val = pickle.load(open(\"valtokens_dataset1.pkl\", \"rb\"))\n",
    "bigram_test = pickle.load(open(\"testtokens_dataset1.pkl\", \"rb\"))\n",
    "train_labeldf = pickle.load(open(\"traintokens_labels.pkl\", \"rb\") )\n",
    "val_labels = pickle.load(open(\"valtokens_labels.pkl\", \"rb\") )\n",
    "test_labels = pickle.load(open(\"testtokens_labels.pkl\", \"rb\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [20000, 50000, 100000, 200000]\n",
    "for size in sizes:\n",
    "    print(\"Now varying vocab size\" + str(size))\n",
    "    bigram_flattened_traintokens = [j for sub in bigram_train for j in sub]\n",
    "    token2id, id2token = build_vocab(bigram_flattened_traintokens, size)\n",
    "    new_model = BagOfWords(len(id2token), emb_dim)\n",
    "    optimizer = torch.optim.Adam(new_model.parameters(), lr=learning_rate)\n",
    "    train_data_indices = token2index_dataset(bigram_train, token2id)\n",
    "    val_data_indices = token2index_dataset(bigram_val, token2id)\n",
    "    test_data_indices = token2index_dataset(bigram_test,token2id)\n",
    "    BATCH_SIZE = 32\n",
    "    train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "    val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=False)\n",
    "    test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and now varying embedding size \n",
    "bigram_flattened_traintokens = [j for sub in bigram_train for j in sub]\n",
    "\n",
    "sizes = [200, 500, 1000, 5000]\n",
    "for size in sizes:\n",
    "    print(\"Now varying vocab size\" + str(size))\n",
    "    token2id, id2token = build_vocab(bigram_flattened_traintokens, 10000)\n",
    "    new_model = BagOfWords(len(id2token), size)\n",
    "    optimizer = torch.optim.Adam(new_model.parameters(), lr=learning_rate)\n",
    "    train_data_indices = token2index_dataset(bigram_train, token2id)\n",
    "    val_data_indices = token2index_dataset(bigram_val, token2id)\n",
    "    test_data_indices = token2index_dataset(bigram_test,token2id)\n",
    "    BATCH_SIZE = 32\n",
    "    train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "    val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=False)\n",
    "    test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trying with new optimizer \n",
    "bigram_flattened_traintokens = [j for sub in bigram_train for j in sub]\n",
    "token2id, id2token = build_vocab(bigram_flattened_traintokens, 10000)\n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "optimizer = torch.optim.SGD(new_model.parameters(), lr = 0.1, momentum =0.05)\n",
    "train_data_indices = token2index_dataset(bigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(bigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(bigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizing based on learning rate \n",
    "# trying with new optimizer \n",
    "for learning_rate in [0.02, 0.05, 0.1, 0.2, 0.5, 1, 2]:\n",
    "    print(\"learning rate is\")\n",
    "    print(str(learning_rate))\n",
    "    token2id, id2token = build_vocab(bigram_flattened_traintokens, 10000)\n",
    "    new_model = BagOfWords(len(id2token), 100)\n",
    "    optimizer = torch.optim.Adam(new_model.parameters(), lr = learning_rate)\n",
    "    train_data_indices = token2index_dataset(bigram_train, token2id)\n",
    "    val_data_indices = token2index_dataset(bigram_val, token2id)\n",
    "    test_data_indices = token2index_dataset(bigram_test,token2id)\n",
    "    BATCH_SIZE = 32\n",
    "    train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "    val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=False)\n",
    "    test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_flattened_traintokens = [j for sub in bigram_train for j in sub]\n",
    "token2id, id2token = build_vocab(bigram_flattened_traintokens, 200000)\n",
    "new_model = BagOfWords(len(id2token), 200)\n",
    "optimizer = torch.optim.Adam(new_model.parameters(), lr = 0.1)\n",
    "train_data_indices = token2index_dataset(bigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(bigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(bigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_model_routine(train_loader, test_loader, new_model, criterion, optimizer, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, doing with unigrams. \n",
    "bigram_flattened_traintokens = [j for sub in bigram_train for j in sub]\n",
    "token2id, id2token = build_vocab(bigram_flattened_traintokens, 10000)\n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "optimizer = torch.optim.Adam(new_model.parameters(), lr = 0.01, weight_decay=True)\n",
    "train_data_indices = token2index_dataset(bigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(bigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(bigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and now, I'm going to use momentum \n",
    "# Now, doing with unigrams. \n",
    "bigram_flattened_traintokens = [j for sub in bigram_train for j in sub]\n",
    "token2id, id2token = build_vocab(bigram_flattened_traintokens, 10000)\n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "optimizer = torch.optim.Adam(new_model.parameters(), lr = 0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "train_data_indices = token2index_dataset(bigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(bigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(bigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate, scheduler=scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
