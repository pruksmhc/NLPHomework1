{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "# split train data into actual train and validation set. \n",
    "DIRLINK = \"aclImdb\"\n",
    "def random_shuffle(threads, labels):\n",
    "\t\"\"\"\n",
    "\tthis function randomly shuffles the data and labels\n",
    "\t\"\"\"\n",
    "\tthreads = np.array(threads)\n",
    "\tlabels = np.array(labels)\n",
    "\ts = np.arange(labels.shape[0])\n",
    "\tnp.random.shuffle(s)\n",
    "\tthreads = threads[s]\n",
    "\tlabels = labels[s]\n",
    "\treturn threads, labels\n",
    "\n",
    "def get_test_train(traintest):\n",
    "    positive = [open(DIRLINK+\"/\"+traintest+\"/pos/\"+file).read() for file in os.listdir(DIRLINK+\"/\"+traintest+\"/pos/\")]\n",
    "    negative = [open(DIRLINK+\"/\"+traintest+\"/neg/\"+file).read() for file in os.listdir(DIRLINK+\"/\"+traintest+\"/neg/\")]\n",
    "    labels = [1]* len(positive)\n",
    "    data = positive\n",
    "    data.extend(negative)\n",
    "    labels.extend([0]*len(negative))\n",
    "    # shuffle the dataset\n",
    "    return data, labels \n",
    "\n",
    "#train_data, train_labels = get_test_train(\"train\")\n",
    "#test_data, test_labels = get_test_train(\"test\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Split train data into actual train and validation sets\\nLEN_TRAIN = len(train_data)\\nLEN_TEST = len(test_data)\\ntrain_datadf = train_data[:int(round(LEN_TRAIN*0.8))]\\ntrain_labeldf = train_labels[:int(round(LEN_TRAIN*0.8))]\\nval_data = train_data[int(round(LEN_TRAIN*0.8)):]\\nval_labels = train_labels[int(round(LEN_TRAIN*0.8)):]\\n\\nprint (\"Train dataset size is {}\".format(len(train_datadf)))\\nprint (\"Val dataset size is {}\".format(len(val_data)))\\nprint (\"Test dataset size is {}\".format(len(test_data)))\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Split train data into actual train and validation sets\n",
    "LEN_TRAIN = len(train_data)\n",
    "LEN_TEST = len(test_data)\n",
    "train_datadf = train_data[:int(round(LEN_TRAIN*0.8))]\n",
    "train_labeldf = train_labels[:int(round(LEN_TRAIN*0.8))]\n",
    "val_data = train_data[int(round(LEN_TRAIN*0.8)):]\n",
    "val_labels = train_labels[int(round(LEN_TRAIN*0.8)):]\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_datadf)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Random sample from train dataset\\nimport random\\nprint (train_data[random.randint(0, len(train_data) - 1)])\\n#!pip install spacy\\n#!python -m spacy download en_core_web_sm\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Random sample from train dataset\n",
    "import random\n",
    "print (train_data[random.randint(0, len(train_data) - 1)])\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/yadapruksachatkun/miniconda3/envs/nlpclass/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /Users/yadapruksachatkun/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[31mmkl-random 1.0.1 requires cython, which is not installed.\u001b[0m\n",
      "\u001b[31mmkl-fft 1.0.4 requires cython, which is not installed.\u001b[0m\n",
      "\u001b[31mtensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Functions to do with Tokenization\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import spacy\n",
    "import string\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "!pip install nltk\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [str(token).lower() for token in parsed if (str(token) not in punctuations)]\n",
    "\n",
    "def get_ngram(sample, n_gram):\n",
    "    sample = sample.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(sample)-n_gram+1):\n",
    "        output.append(sample[i:i+n_gram])\n",
    "    final = [' '.join(x) for x in output]\n",
    "    return final\n",
    "\n",
    "def strip_html(sample): \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sample)\n",
    "    return cleantext \n",
    "\n",
    "def tokenize_dataset(dataset, name, n_gram):\n",
    "    token_dataset = {i:[] for i in range(1, n_gram + 1)}\n",
    "    all_datasets = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        sample = str(sample)\n",
    "        #sample = strip_html(sample)\n",
    "        for i in range(1, n_gram+1):\n",
    "            sample_after = get_ngram(sample, i)\n",
    "            tokens = lower_case_remove_punc(sample_after)\n",
    "            token_dataset[i].append(tokens)\n",
    "    for i in range(1, n_gram+1):\n",
    "        pickle.dump(token_dataset[i],open(name+\"tokens_dataset\"+str(i)+\".pkl\", \"wb\") ) \n",
    "        all_datasets.append(token_dataset[i])\n",
    "    return all_datasets\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(dataset):\n",
    "    new_dataset = []\n",
    "    for review in dataset:\n",
    "        new_review = []\n",
    "        for word in review:\n",
    "            new_review.append(wordnet_lemmatizer.lemmatize(word))\n",
    "        new_dataset.append(\"\".join(new_review))\n",
    "    return new_dataset\n",
    "\n",
    "#tokenized_train = tokenize_dataset(lemmatize(train_datadf), \"trainlemma\", 2)\n",
    "#tokenized_val = tokenize_dataset(lemmatize(val_data) , \"vallemma\", 2)\n",
    "#tokenized_test = tokenize_dataset(lemmatize(test_data), \"testlemma\", 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nunigram_train = pickle.load(open(\"trainlemmatokens_dataset2.pkl\", \"rb\"))\\nunigram_val = pickle.load(open(\"vallemmatokens_dataset2.pkl\", \"rb\"))\\nunigram_test = pickle.load(open(\"testlemmatokens_dataset2.pkl\", \"rb\"))\\ntrain_labeldf = pickle.load(open(\"traintokens_labels.pkl\", \"rb\") )\\nval_labels = pickle.load(open(\"valtokens_labels.pkl\", \"rb\") )\\ntest_labels = pickle.load(open(\"testtokens_labels.pkl\", \"rb\") )\\n\\npickle.dump(train_labeldf,open(\"traintokens_labels.pkl\", \"wb\") ) \\npickle.dump(val_labels,open(\"valtokens_labels.pkl\", \"wb\") ) \\npickle.dump(test_labels,open(\"testtokens_labels.pkl\", \"wb\") ) \\n# these are the labels\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "unigram_train = pickle.load(open(\"trainlemmatokens_dataset2.pkl\", \"rb\"))\n",
    "unigram_val = pickle.load(open(\"vallemmatokens_dataset2.pkl\", \"rb\"))\n",
    "unigram_test = pickle.load(open(\"testlemmatokens_dataset2.pkl\", \"rb\"))\n",
    "train_labeldf = pickle.load(open(\"traintokens_labels.pkl\", \"rb\") )\n",
    "val_labels = pickle.load(open(\"valtokens_labels.pkl\", \"rb\") )\n",
    "test_labels = pickle.load(open(\"testtokens_labels.pkl\", \"rb\") )\n",
    "\n",
    "pickle.dump(train_labeldf,open(\"traintokens_labels.pkl\", \"wb\") ) \n",
    "pickle.dump(val_labels,open(\"valtokens_labels.pkl\", \"wb\") ) \n",
    "pickle.dump(test_labels,open(\"testtokens_labels.pkl\", \"wb\") ) \n",
    "# these are the labels\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "//I'm doing this to be able to move cells up or down easily. \n",
       "IPython.keyboard_manager.command_shortcuts.add_shortcut('Ctrl-k', {\n",
       "    help : 'move up selected cells',\n",
       "    help_index : 'jupyter-notebook:move-selection-up',\n",
       "    handler : function (event) {\n",
       "        IPython.notebook.move_selection_up();\n",
       "        return false;\n",
       "    }}\n",
       ");\n",
       "\n",
       "IPython.keyboard_manager.command_shortcuts.add_shortcut('Ctrl-j', {\n",
       "    help : 'move down selected cells',\n",
       "    help_index : 'jupyter-notebook:move-selection-down',\n",
       "    handler :  function (event) {\n",
       "        IPython.notebook.move_selection_down();\n",
       "        return false;\n",
       "    }}\n",
       ");"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "//I'm doing this to be able to move cells up or down easily. \n",
    "IPython.keyboard_manager.command_shortcuts.add_shortcut('Ctrl-k', {\n",
    "    help : 'move up selected cells',\n",
    "    help_index : 'jupyter-notebook:move-selection-up',\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.move_selection_up();\n",
    "        return false;\n",
    "    }}\n",
    ");\n",
    "\n",
    "IPython.keyboard_manager.command_shortcuts.add_shortcut('Ctrl-j', {\n",
    "    help : 'move down selected cells',\n",
    "    help_index : 'jupyter-notebook:move-selection-down',\n",
    "    handler :  function (event) {\n",
    "        IPython.notebook.move_selection_down();\n",
    "        return false;\n",
    "    }}\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# find the 1000 most common words and subwords. \n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "# flatten the toeknized train to become list of all the tokens\n",
    "#unigram_flattened_traintokens = [j for sub in unigram_train for j in sub]\n",
    "\n",
    "#zoken2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token2id, id2token = build_vocab(unigram_flattened_traintokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_data_indices = token2index_dataset(unigram_train)\\nval_data_indices = token2index_dataset(unigram_val)\\ntest_data_indices = token2index_dataset(unigram_test)\\n\\n# double checking\\nprint (\"Train dataset size is {}\".format(len(train_data_indices)))\\nprint (\"Val dataset size is {}\".format(len(val_data_indices)))\\nprint (\"Test dataset size is {}\".format(len(test_data_indices)))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\"\"\"\n",
    "train_data_indices = token2index_dataset(unigram_train)\n",
    "val_data_indices = token2index_dataset(unigram_val)\n",
    "test_data_indices = token2index_dataset(unigram_test)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(len(train_labeldf))\\nprint(len(val_labels))\\nprint(len(test_labels))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(len(train_labeldf))\n",
    "print(len(val_labels))\n",
    "print(len(test_labels))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "# change this based on whicehver n-gram file you're reading in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBATCH_SIZE = 32\\ntrain_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \\n                                           batch_size=BATCH_SIZE,\\n                                           collate_fn=moviegroup_collate_func,\\n                                           shuffle=True)\\n\\nval_dataset = MovieGroupDataset(val_data_indices, val_labels)\\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset, \\n                                           batch_size=BATCH_SIZE,\\n                                           collate_fn=moviegroup_collate_func,\\n                                           shuffle=True)\\n\\ntest_dataset = MovieGroupDataset(test_data_indices, test_labels)\\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \\n                                           batch_size=BATCH_SIZE,\\n                                           collate_fn=moviegroup_collate_func,\\n                                           shuffle=False)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def moviegroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "\"\"\"\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        # view basically reshapes it, so this averages it out. \n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "#model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 3 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def test_model_routine(train_loader, val_loader, model, criterion, optimizer, num_epochs, learning_rate, scheduler=None):\n",
    "    acc_per_step_val = []\n",
    "    acc_per_step_train = []\n",
    "    for epoch in range(num_epochs):\n",
    "        acc = []\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model) \n",
    "                train_acc = test_model(train_loader, model)\n",
    "                acc.append(val_acc)\n",
    "                acc_per_step_val.append(val_acc)\n",
    "                acc_per_step_train.append(train_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Train Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc, train_acc))\n",
    "        #scheduler.step(loss)\n",
    "        print(\"Average accuracy is\"+ str(np.mean(acc)))\n",
    "    print(\"total average accuarcies validation\")\n",
    "    print(acc_per_step_val)\n",
    "    print(\"total accuracies train\")\n",
    "    print(acc_per_step_train)\n",
    "    return acc_per_step_val, acc_per_step_train, model\n",
    "#test_model_routine(train_loader, val_loader, model, criterion, optimizer, num_epochs, learning_rate)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NOW, LET'S USE ONLY UNIGRAM\n",
    "unigram_train = pickle.load(open(\"traintokens_dataset1.pkl\", \"rb\"))\n",
    "unigram_val = pickle.load(open(\"valtokens_dataset1.pkl\", \"rb\"))\n",
    "unigram_test = pickle.load(open(\"testtokens_dataset1.pkl\", \"rb\"))\n",
    "train_labeldf = pickle.load(open(\"traintokens_labels.pkl\", \"rb\") )\n",
    "val_labels = pickle.load(open(\"valtokens_labels.pkl\", \"rb\") )\n",
    "test_labels = pickle.load(open(\"testtokens_labels.pkl\", \"rb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now varying vocab size200000\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 93.56, Train Acc: 86.26\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 97.02, Train Acc: 90.245\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 97.16, Train Acc: 93.215\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 97.14, Train Acc: 94.9\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 98.3, Train Acc: 96.58\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 98.32, Train Acc: 98.06\n",
      "Average accuracy is96.9166666667\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 97.66, Train Acc: 98.545\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 98.3, Train Acc: 98.825\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 98.66, Train Acc: 98.85\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 98.06, Train Acc: 99.33\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 98.04, Train Acc: 99.595\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 98.28, Train Acc: 99.875\n",
      "Average accuracy is98.1666666667\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 97.96, Train Acc: 99.935\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 98.26, Train Acc: 99.95\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 97.82, Train Acc: 99.98\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 97.7, Train Acc: 99.97\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 98.0, Train Acc: 99.99\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 97.66, Train Acc: 100.0\n",
      "Average accuracy is97.9\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 97.78, Train Acc: 100.0\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 98.0, Train Acc: 100.0\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 97.76, Train Acc: 100.0\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 97.96, Train Acc: 100.0\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 98.0, Train Acc: 100.0\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 97.94, Train Acc: 100.0\n",
      "Average accuracy is97.9066666667\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 97.88, Train Acc: 100.0\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 97.88, Train Acc: 100.0\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 97.88, Train Acc: 100.0\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 97.86, Train Acc: 100.0\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 97.98, Train Acc: 100.0\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 97.94, Train Acc: 100.0\n",
      "Average accuracy is97.9033333333\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 97.94, Train Acc: 100.0\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 97.88, Train Acc: 100.0\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 97.88, Train Acc: 100.0\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 98.0, Train Acc: 100.0\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 97.88, Train Acc: 100.0\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 97.86, Train Acc: 100.0\n",
      "Average accuracy is97.9066666667\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 97.88, Train Acc: 100.0\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 97.86, Train Acc: 100.0\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 97.96, Train Acc: 100.0\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 98.02, Train Acc: 100.0\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 97.9, Train Acc: 100.0\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 97.96, Train Acc: 100.0\n",
      "Average accuracy is97.93\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 97.92, Train Acc: 100.0\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 97.92, Train Acc: 100.0\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 97.88, Train Acc: 100.0\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 97.88, Train Acc: 100.0\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 97.86, Train Acc: 100.0\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 97.86, Train Acc: 100.0\n",
      "Average accuracy is97.8866666667\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 98.0, Train Acc: 100.0\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 97.94, Train Acc: 100.0\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 97.92, Train Acc: 100.0\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 97.82, Train Acc: 100.0\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 97.94, Train Acc: 100.0\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 97.96, Train Acc: 100.0\n",
      "Average accuracy is97.93\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 97.96, Train Acc: 100.0\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 97.94, Train Acc: 100.0\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 97.94, Train Acc: 100.0\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 97.92, Train Acc: 100.0\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 98.0, Train Acc: 100.0\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 97.84, Train Acc: 100.0\n",
      "Average accuracy is97.9333333333\n",
      "total average accuarcies validation\n",
      "[93.56, 97.02, 97.16, 97.14, 98.3, 98.32, 97.66, 98.3, 98.66, 98.06, 98.04, 98.28, 97.96, 98.26, 97.82, 97.7, 98.0, 97.66, 97.78, 98.0, 97.76, 97.96, 98.0, 97.94, 97.88, 97.88, 97.88, 97.86, 97.98, 97.94, 97.94, 97.88, 97.88, 98.0, 97.88, 97.86, 97.88, 97.86, 97.96, 98.02, 97.9, 97.96, 97.92, 97.92, 97.88, 97.88, 97.86, 97.86, 98.0, 97.94, 97.92, 97.82, 97.94, 97.96, 97.96, 97.94, 97.94, 97.92, 98.0, 97.84]\n",
      "total accuracies train\n",
      "[86.26, 90.245, 93.215, 94.9, 96.58, 98.06, 98.545, 98.825, 98.85, 99.33, 99.595, 99.875, 99.935, 99.95, 99.98, 99.97, 99.99, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n"
     ]
    }
   ],
   "source": [
    "sizes = [20000, 50000, 100000, 200000]\n",
    "for size in sizes:\n",
    "    print(\"Now varying vocab size\" + str(size))\n",
    "    unigram_flattened_traintokens = [j for sub in unigram_train for j in sub]\n",
    "    token2id, id2token = build_vocab(unigram_flattened_traintokens, size)\n",
    "    new_model = BagOfWords(len(id2token), emb_dim)\n",
    "    optimizer = torch.optim.Adam(new_model.parameters(), lr=learning_rate)\n",
    "    train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "    val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "    test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "    BATCH_SIZE = 32\n",
    "    train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "    val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=False)\n",
    "    test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now varying vocab size200\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 59.12, Train Acc: 81.975\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 78.46, Train Acc: 86.89\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 77.3, Train Acc: 89.295\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 83.88, Train Acc: 90.28\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 79.38, Train Acc: 91.905\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 72.16, Train Acc: 92.085\n",
      "Average accuracy is75.05\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 75.32, Train Acc: 93.01\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 80.28, Train Acc: 93.865\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 79.36, Train Acc: 94.24\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 80.42, Train Acc: 94.83\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 67.28, Train Acc: 92.99\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 84.12, Train Acc: 94.76\n",
      "Average accuracy is77.7966666667\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 84.64, Train Acc: 95.205\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 83.78, Train Acc: 95.26\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 78.22, Train Acc: 96.12\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 76.18, Train Acc: 96.355\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 69.94, Train Acc: 95.23\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 83.3, Train Acc: 96.31\n",
      "Average accuracy is79.3433333333\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 82.42, Train Acc: 96.855\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 72.98, Train Acc: 96.775\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 73.8, Train Acc: 96.775\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 78.46, Train Acc: 97.23\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 76.12, Train Acc: 97.27\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 85.42, Train Acc: 95.805\n",
      "Average accuracy is78.2\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 74.6, Train Acc: 97.62\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 82.28, Train Acc: 97.07\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 76.72, Train Acc: 97.845\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 77.22, Train Acc: 97.955\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 71.66, Train Acc: 97.43\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 78.1, Train Acc: 98.285\n",
      "Average accuracy is76.7633333333\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 79.74, Train Acc: 98.03\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 74.94, Train Acc: 98.51\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 74.42, Train Acc: 98.45\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 78.72, Train Acc: 98.48\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 76.74, Train Acc: 98.545\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 78.54, Train Acc: 98.62\n",
      "Average accuracy is77.1833333333\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 77.76, Train Acc: 98.785\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 76.64, Train Acc: 98.915\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 80.46, Train Acc: 98.395\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 74.12, Train Acc: 98.74\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 76.92, Train Acc: 98.9\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 71.82, Train Acc: 98.255\n",
      "Average accuracy is76.2866666667\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 75.86, Train Acc: 98.995\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 77.0, Train Acc: 99.105\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 77.78, Train Acc: 99.07\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 70.24, Train Acc: 97.865\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 81.5, Train Acc: 97.87\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 76.72, Train Acc: 98.935\n",
      "Average accuracy is76.5166666667\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 76.96, Train Acc: 99.15\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 76.46, Train Acc: 99.29\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 75.16, Train Acc: 99.225\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 79.04, Train Acc: 98.885\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 69.44, Train Acc: 97.73\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 78.3, Train Acc: 99.055\n",
      "Average accuracy is75.8933333333\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 76.08, Train Acc: 99.335\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 70.34, Train Acc: 98.47\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 72.82, Train Acc: 98.925\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 77.28, Train Acc: 99.17\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 72.64, Train Acc: 99.08\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 77.44, Train Acc: 99.215\n",
      "Average accuracy is74.4333333333\n",
      "total average accuarcies\n",
      "[75.049999999999997, 77.796666666666667, 79.343333333333348, 78.200000000000003, 76.763333333333335, 77.183333333333351, 76.286666666666676, 76.516666666666666, 75.893333333333331, 74.433333333333323]\n",
      "Now varying vocab size500\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 51.38, Train Acc: 80.685\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 77.8, Train Acc: 87.335\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 63.3, Train Acc: 87.315\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 70.86, Train Acc: 90.065\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 82.92, Train Acc: 91.49\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 77.48, Train Acc: 92.205\n",
      "Average accuracy is70.6233333333\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 82.28, Train Acc: 93.01\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 83.62, Train Acc: 92.865\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 75.34, Train Acc: 93.615\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 85.96, Train Acc: 92.87\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 74.9, Train Acc: 94.435\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 72.96, Train Acc: 94.39\n",
      "Average accuracy is79.1766666667\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 72.7, Train Acc: 94.915\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 81.18, Train Acc: 95.115\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 75.94, Train Acc: 95.6\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 70.9, Train Acc: 94.935\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 75.9, Train Acc: 96.34\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 82.78, Train Acc: 96.18\n",
      "Average accuracy is76.5666666667\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 79.28, Train Acc: 97.06\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 71.96, Train Acc: 96.195\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 73.26, Train Acc: 96.335\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 81.64, Train Acc: 96.415\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 79.0, Train Acc: 97.06\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 78.8, Train Acc: 97.4\n",
      "Average accuracy is77.3233333333\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 78.58, Train Acc: 97.765\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 78.94, Train Acc: 97.655\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 74.28, Train Acc: 97.535\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 74.28, Train Acc: 97.6\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 76.42, Train Acc: 97.905\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 79.88, Train Acc: 97.75\n",
      "Average accuracy is77.0633333333\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 74.84, Train Acc: 98.27\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 78.52, Train Acc: 98.255\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 73.94, Train Acc: 98.185\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 82.36, Train Acc: 97.275\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 79.86, Train Acc: 97.74\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 75.34, Train Acc: 98.2\n",
      "Average accuracy is77.4766666667\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 73.64, Train Acc: 98.27\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 76.76, Train Acc: 98.395\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 78.74, Train Acc: 98.42\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 78.06, Train Acc: 98.49\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 82.34, Train Acc: 97.37\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 78.42, Train Acc: 98.42\n",
      "Average accuracy is77.9933333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/10], Step: [101/625], Validation Acc: 72.5, Train Acc: 98.26\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 73.24, Train Acc: 98.575\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 76.1, Train Acc: 99.005\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 75.06, Train Acc: 98.84\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 79.46, Train Acc: 98.53\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 72.62, Train Acc: 98.63\n",
      "Average accuracy is74.83\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 75.78, Train Acc: 99.23\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 71.92, Train Acc: 98.395\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 76.98, Train Acc: 99.25\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 72.86, Train Acc: 98.795\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 73.1, Train Acc: 98.695\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 72.72, Train Acc: 98.91\n",
      "Average accuracy is73.8933333333\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 73.08, Train Acc: 98.915\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 76.6, Train Acc: 99.315\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 77.44, Train Acc: 99.15\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 76.1, Train Acc: 99.16\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 73.28, Train Acc: 99.06\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 78.54, Train Acc: 98.715\n",
      "Average accuracy is75.84\n",
      "total average accuarcies\n",
      "[70.623333333333349, 79.176666666666662, 76.566666666666663, 77.323333333333338, 77.063333333333333, 77.476666666666674, 77.993333333333325, 74.829999999999998, 73.893333333333331, 75.840000000000003]\n",
      "Now varying vocab size1000\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 58.6, Train Acc: 82.09\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 64.26, Train Acc: 86.065\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 56.84, Train Acc: 85.22\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 85.76, Train Acc: 89.295\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 81.98, Train Acc: 91.27\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 80.7, Train Acc: 92.525\n",
      "Average accuracy is71.3566666667\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 77.36, Train Acc: 93.15\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 76.72, Train Acc: 92.925\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 74.46, Train Acc: 93.325\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 75.6, Train Acc: 94.075\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 79.38, Train Acc: 94.355\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 73.84, Train Acc: 94.345\n",
      "Average accuracy is76.2266666667\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 79.1, Train Acc: 95.625\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 74.66, Train Acc: 94.975\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 80.96, Train Acc: 95.425\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 79.78, Train Acc: 94.94\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 81.22, Train Acc: 94.945\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 83.4, Train Acc: 94.625\n",
      "Average accuracy is79.8533333333\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 79.42, Train Acc: 96.79\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 82.18, Train Acc: 95.885\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 77.32, Train Acc: 96.52\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 73.36, Train Acc: 96.15\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 79.1, Train Acc: 96.435\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 75.24, Train Acc: 97.09\n",
      "Average accuracy is77.77\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 76.3, Train Acc: 97.49\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 77.88, Train Acc: 97.61\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 74.18, Train Acc: 97.16\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 76.28, Train Acc: 97.36\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 77.36, Train Acc: 97.26\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 79.88, Train Acc: 96.405\n",
      "Average accuracy is76.98\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 78.32, Train Acc: 97.98\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 77.12, Train Acc: 97.945\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 72.9, Train Acc: 97.315\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 78.84, Train Acc: 97.66\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 77.94, Train Acc: 97.495\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 71.8, Train Acc: 97.17\n",
      "Average accuracy is76.1533333333\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 77.52, Train Acc: 98.47\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 77.38, Train Acc: 98.285\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 85.46, Train Acc: 95.1\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 79.46, Train Acc: 97.94\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 77.42, Train Acc: 98.3\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 76.22, Train Acc: 98.285\n",
      "Average accuracy is78.91\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 77.7, Train Acc: 98.59\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 71.66, Train Acc: 98.145\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 80.72, Train Acc: 97.8\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 66.26, Train Acc: 95.74\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 72.68, Train Acc: 97.99\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 78.58, Train Acc: 98.25\n",
      "Average accuracy is74.6\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 74.92, Train Acc: 98.745\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 77.1, Train Acc: 98.74\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 73.96, Train Acc: 98.545\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 78.8, Train Acc: 98.43\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 74.0, Train Acc: 98.41\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 77.74, Train Acc: 98.61\n",
      "Average accuracy is76.0866666667\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 79.36, Train Acc: 98.62\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 74.56, Train Acc: 99.135\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 80.26, Train Acc: 98.415\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 75.54, Train Acc: 98.795\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 77.38, Train Acc: 98.825\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 77.1, Train Acc: 98.98\n",
      "Average accuracy is77.3666666667\n",
      "total average accuarcies\n",
      "[71.356666666666669, 76.226666666666674, 79.853333333333339, 77.769999999999996, 76.980000000000004, 76.153333333333336, 78.909999999999982, 74.600000000000009, 76.086666666666659, 77.366666666666674]\n",
      "Now varying vocab size5000\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 92.56, Train Acc: 73.8\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 86.14, Train Acc: 82.31\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 50.86, Train Acc: 81.09\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 73.02, Train Acc: 88.555\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 84.66, Train Acc: 87.69\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 74.72, Train Acc: 89.395\n",
      "Average accuracy is76.9933333333\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 80.24, Train Acc: 90.755\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 70.86, Train Acc: 90.35\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 75.06, Train Acc: 89.285\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 73.3, Train Acc: 90.09\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 82.98, Train Acc: 89.695\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 77.58, Train Acc: 92.115\n",
      "Average accuracy is76.67\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 83.82, Train Acc: 92.31\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 76.02, Train Acc: 93.075\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 76.62, Train Acc: 92.515\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 72.24, Train Acc: 93.24\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 83.06, Train Acc: 92.19\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 76.56, Train Acc: 93.12\n",
      "Average accuracy is78.0533333333\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 76.32, Train Acc: 94.235\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 81.68, Train Acc: 93.375\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 79.68, Train Acc: 93.91\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 71.32, Train Acc: 94.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/10], Step: [501/625], Validation Acc: 79.96, Train Acc: 94.475\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 71.66, Train Acc: 94.55\n",
      "Average accuracy is76.77\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 82.54, Train Acc: 93.89\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 82.72, Train Acc: 93.715\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 70.1, Train Acc: 93.76\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 76.24, Train Acc: 94.935\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 78.02, Train Acc: 94.44\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 77.42, Train Acc: 94.96\n",
      "Average accuracy is77.84\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 73.2, Train Acc: 95.885\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 70.88, Train Acc: 94.77\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 63.68, Train Acc: 92.265\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 74.82, Train Acc: 96.03\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 76.46, Train Acc: 96.12\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 73.32, Train Acc: 95.09\n",
      "Average accuracy is72.06\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 78.22, Train Acc: 96.195\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 68.34, Train Acc: 94.08\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 80.82, Train Acc: 95.44\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 72.4, Train Acc: 96.015\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 76.86, Train Acc: 95.84\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 79.5, Train Acc: 95.675\n",
      "Average accuracy is76.0233333333\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 78.06, Train Acc: 97.195\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 75.56, Train Acc: 96.805\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 79.64, Train Acc: 96.585\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 73.8, Train Acc: 96.18\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 81.18, Train Acc: 95.73\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 79.48, Train Acc: 96.815\n",
      "Average accuracy is77.9533333333\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 71.76, Train Acc: 96.48\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 77.0, Train Acc: 97.24\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 73.9, Train Acc: 96.85\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 72.1, Train Acc: 96.7\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 74.98, Train Acc: 97.005\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 70.08, Train Acc: 95.72\n",
      "Average accuracy is73.3033333333\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 72.14, Train Acc: 97.38\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 72.88, Train Acc: 97.28\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 77.86, Train Acc: 97.12\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 77.46, Train Acc: 97.155\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 78.5, Train Acc: 96.825\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 80.74, Train Acc: 95.995\n",
      "Average accuracy is76.5966666667\n",
      "total average accuarcies\n",
      "[76.993333333333339, 76.670000000000002, 78.053333333333327, 76.769999999999996, 77.839999999999989, 72.059999999999988, 76.023333333333326, 77.953333333333333, 73.303333333333327, 76.596666666666664]\n"
     ]
    }
   ],
   "source": [
    "# and now varying embedding size \n",
    "\n",
    "sizes = [200, 500, 1000, 5000]\n",
    "for size in sizes:\n",
    "    print(\"Now varying vocab size\" + str(size))\n",
    "    token2id, id2token = build_vocab(unigram_flattened_traintokens, 10000)\n",
    "    new_model = BagOfWords(len(id2token), size)\n",
    "    optimizer = torch.optim.Adam(new_model.parameters(), lr=learning_rate)\n",
    "    train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "    val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "    test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "    BATCH_SIZE = 32\n",
    "    train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "    val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=False)\n",
    "    test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 0.04, Train Acc: 62.505\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 1.64, Train Acc: 62.825\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 1.54, Train Acc: 62.835\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 1.96, Train Acc: 63.0\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 3.98, Train Acc: 63.435\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 6.18, Train Acc: 63.93\n",
      "Average accuracy is2.55666666667\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 10.98, Train Acc: 64.835\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 16.74, Train Acc: 65.96\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 15.1, Train Acc: 65.77\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 30.74, Train Acc: 67.86\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 14.94, Train Acc: 65.895\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 18.18, Train Acc: 66.57\n",
      "Average accuracy is17.78\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 26.34, Train Acc: 67.955\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 21.72, Train Acc: 67.145\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 28.54, Train Acc: 68.34\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 23.1, Train Acc: 67.615\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 33.02, Train Acc: 69.075\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 27.58, Train Acc: 68.45\n",
      "Average accuracy is26.7166666667\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 35.04, Train Acc: 69.43\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 31.22, Train Acc: 69.135\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 30.14, Train Acc: 68.995\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 31.6, Train Acc: 69.305\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 45.94, Train Acc: 69.98\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 25.52, Train Acc: 68.415\n",
      "Average accuracy is33.2433333333\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 29.66, Train Acc: 69.08\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 42.36, Train Acc: 70.105\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 23.12, Train Acc: 67.975\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 25.4, Train Acc: 68.51\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 31.24, Train Acc: 69.48\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 35.44, Train Acc: 70.16\n",
      "Average accuracy is31.2033333333\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 32.8, Train Acc: 69.94\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 34.36, Train Acc: 70.16\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 44.16, Train Acc: 70.68\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 38.9, Train Acc: 70.635\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 39.96, Train Acc: 70.845\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 31.96, Train Acc: 69.885\n",
      "Average accuracy is37.0233333333\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 32.4, Train Acc: 70.065\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 52.32, Train Acc: 70.94\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 40.26, Train Acc: 71.105\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 43.38, Train Acc: 71.145\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 44.28, Train Acc: 71.26\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 35.72, Train Acc: 70.795\n",
      "Average accuracy is41.3933333333\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 32.74, Train Acc: 70.255\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 36.5, Train Acc: 70.925\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 39.82, Train Acc: 71.425\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 34.26, Train Acc: 70.72\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 40.02, Train Acc: 71.54\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 33.46, Train Acc: 70.655\n",
      "Average accuracy is36.1333333333\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 41.6, Train Acc: 71.7\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 45.04, Train Acc: 71.76\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 40.42, Train Acc: 71.8\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 40.26, Train Acc: 71.82\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 46.68, Train Acc: 72.045\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 44.32, Train Acc: 72.025\n",
      "Average accuracy is43.0533333333\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 47.44, Train Acc: 72.14\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 41.88, Train Acc: 71.935\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 56.6, Train Acc: 72.035\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 38.76, Train Acc: 71.82\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 44.74, Train Acc: 72.265\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 42.74, Train Acc: 72.285\n",
      "Average accuracy is45.36\n",
      "total average accuarcies\n",
      "[2.5566666666666666, 17.780000000000001, 26.716666666666669, 33.243333333333332, 31.203333333333333, 37.023333333333333, 41.393333333333331, 36.133333333333333, 43.053333333333335, 45.359999999999992]\n"
     ]
    }
   ],
   "source": [
    "# trying with new optimizer \n",
    "token2id, id2token = build_vocab(unigram_flattened_traintokens, 10000)\n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "optimizer = torch.optim.SGD(new_model.parameters(), lr = 0.1, momentum =0.05)\n",
    "train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is\n",
      "0.02\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 56.74, Train Acc: 82.58\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 64.42, Train Acc: 86.45\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 66.1, Train Acc: 87.875\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 82.6, Train Acc: 90.83\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 76.16, Train Acc: 92.045\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 83.34, Train Acc: 92.96\n",
      "Average accuracy is71.56\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 67.22, Train Acc: 91.315\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 78.84, Train Acc: 93.765\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 73.34, Train Acc: 93.46\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 73.64, Train Acc: 94.015\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 75.64, Train Acc: 94.89\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 72.16, Train Acc: 94.67\n",
      "Average accuracy is73.4733333333\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 80.46, Train Acc: 95.71\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 84.1, Train Acc: 94.755\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 77.3, Train Acc: 95.695\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 75.0, Train Acc: 95.82\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 77.74, Train Acc: 96.275\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 78.18, Train Acc: 96.81\n",
      "Average accuracy is78.7966666667\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 81.06, Train Acc: 96.81\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 78.24, Train Acc: 96.84\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 75.84, Train Acc: 96.47\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 74.82, Train Acc: 96.895\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 74.6, Train Acc: 97.195\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 67.78, Train Acc: 95.36\n",
      "Average accuracy is75.39\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 80.7, Train Acc: 97.66\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 75.78, Train Acc: 97.755\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 82.74, Train Acc: 96.6\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 76.2, Train Acc: 97.665\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 73.64, Train Acc: 97.525\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 82.22, Train Acc: 97.255\n",
      "Average accuracy is78.5466666667\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 76.72, Train Acc: 98.44\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 76.98, Train Acc: 98.39\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 75.24, Train Acc: 98.245\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 78.66, Train Acc: 98.335\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 76.98, Train Acc: 98.255\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 74.24, Train Acc: 98.52\n",
      "Average accuracy is76.47\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 77.5, Train Acc: 98.79\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 79.56, Train Acc: 98.57\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 77.82, Train Acc: 98.675\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 81.9, Train Acc: 97.265\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 73.24, Train Acc: 98.335\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 75.46, Train Acc: 98.9\n",
      "Average accuracy is77.58\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 80.7, Train Acc: 98.18\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 73.64, Train Acc: 98.915\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 75.24, Train Acc: 99.01\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 69.74, Train Acc: 97.845\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 76.24, Train Acc: 98.935\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 79.94, Train Acc: 98.425\n",
      "Average accuracy is75.9166666667\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 75.88, Train Acc: 99.235\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 77.22, Train Acc: 99.235\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 74.12, Train Acc: 99.1\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 74.94, Train Acc: 99.015\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 81.2, Train Acc: 97.74\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 74.96, Train Acc: 99.02\n",
      "Average accuracy is76.3866666667\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 78.12, Train Acc: 99.03\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 74.22, Train Acc: 99.005\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 75.42, Train Acc: 99.255\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 75.74, Train Acc: 99.315\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 78.7, Train Acc: 98.965\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 75.86, Train Acc: 99.325\n",
      "Average accuracy is76.3433333333\n",
      "total average accuarcies validation\n",
      "[56.74, 64.42, 66.1, 82.6, 76.16, 83.34, 67.22, 78.84, 73.34, 73.64, 75.64, 72.16, 80.46, 84.1, 77.3, 75.0, 77.74, 78.18, 81.06, 78.24, 75.84, 74.82, 74.6, 67.78, 80.7, 75.78, 82.74, 76.2, 73.64, 82.22, 76.72, 76.98, 75.24, 78.66, 76.98, 74.24, 77.5, 79.56, 77.82, 81.9, 73.24, 75.46, 80.7, 73.64, 75.24, 69.74, 76.24, 79.94, 75.88, 77.22, 74.12, 74.94, 81.2, 74.96, 78.12, 74.22, 75.42, 75.74, 78.7, 75.86]\n",
      "total accuracies train\n",
      "[82.58, 86.45, 87.875, 90.83, 92.045, 92.96, 91.315, 93.765, 93.46, 94.015, 94.89, 94.67, 95.71, 94.755, 95.695, 95.82, 96.275, 96.81, 96.81, 96.84, 96.47, 96.895, 97.195, 95.36, 97.66, 97.755, 96.6, 97.665, 97.525, 97.255, 98.44, 98.39, 98.245, 98.335, 98.255, 98.52, 98.79, 98.57, 98.675, 97.265, 98.335, 98.9, 98.18, 98.915, 99.01, 97.845, 98.935, 98.425, 99.235, 99.235, 99.1, 99.015, 97.74, 99.02, 99.03, 99.005, 99.255, 99.315, 98.965, 99.325]\n",
      "learning rate is\n",
      "0.05\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 80.04, Train Acc: 84.065\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 55.1, Train Acc: 84.375\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 66.54, Train Acc: 88.335\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 79.82, Train Acc: 90.78\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 69.44, Train Acc: 90.065\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 79.1, Train Acc: 92.985\n",
      "Average accuracy is71.6733333333\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 82.34, Train Acc: 92.83\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 78.24, Train Acc: 93.575\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 70.14, Train Acc: 92.615\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 75.34, Train Acc: 94.425\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 77.48, Train Acc: 94.7\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 78.98, Train Acc: 95.095\n",
      "Average accuracy is77.0866666667\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 75.14, Train Acc: 94.89\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 71.86, Train Acc: 94.245\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 78.54, Train Acc: 94.985\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 79.76, Train Acc: 95.775\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 67.02, Train Acc: 93.795\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 72.7, Train Acc: 95.56\n",
      "Average accuracy is74.17\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 74.68, Train Acc: 96.385\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 80.86, Train Acc: 95.825\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 68.98, Train Acc: 94.635\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 77.06, Train Acc: 96.585\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 72.18, Train Acc: 96.215\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 75.38, Train Acc: 96.965\n",
      "Average accuracy is74.8566666667\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 81.3, Train Acc: 96.52\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 77.38, Train Acc: 96.88\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 77.92, Train Acc: 97.165\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 75.56, Train Acc: 97.27\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 73.22, Train Acc: 96.86\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 82.58, Train Acc: 94.865\n",
      "Average accuracy is77.9933333333\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 75.98, Train Acc: 97.51\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 75.64, Train Acc: 97.52\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 73.84, Train Acc: 97.15\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 77.26, Train Acc: 97.525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/10], Step: [501/625], Validation Acc: 71.82, Train Acc: 96.475\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 74.54, Train Acc: 98.1\n",
      "Average accuracy is74.8466666667\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 76.24, Train Acc: 97.855\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 74.28, Train Acc: 97.775\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 79.08, Train Acc: 97.325\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 75.98, Train Acc: 97.83\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 70.36, Train Acc: 97.05\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 77.12, Train Acc: 97.925\n",
      "Average accuracy is75.51\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 73.24, Train Acc: 97.83\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 78.94, Train Acc: 97.575\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 70.86, Train Acc: 97.26\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 76.3, Train Acc: 98.24\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 76.86, Train Acc: 98.62\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 70.9, Train Acc: 97.74\n",
      "Average accuracy is74.5166666667\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 80.74, Train Acc: 97.77\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 82.04, Train Acc: 97.315\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 73.0, Train Acc: 98.235\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 75.5, Train Acc: 98.525\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 69.9, Train Acc: 97.36\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 79.66, Train Acc: 97.73\n",
      "Average accuracy is76.8066666667\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 75.7, Train Acc: 98.745\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 73.98, Train Acc: 98.61\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 72.12, Train Acc: 97.735\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 75.36, Train Acc: 98.065\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 76.38, Train Acc: 98.585\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 76.96, Train Acc: 98.615\n",
      "Average accuracy is75.0833333333\n",
      "total average accuarcies validation\n",
      "[80.04, 55.1, 66.54, 79.82, 69.44, 79.1, 82.34, 78.24, 70.14, 75.34, 77.48, 78.98, 75.14, 71.86, 78.54, 79.76, 67.02, 72.7, 74.68, 80.86, 68.98, 77.06, 72.18, 75.38, 81.3, 77.38, 77.92, 75.56, 73.22, 82.58, 75.98, 75.64, 73.84, 77.26, 71.82, 74.54, 76.24, 74.28, 79.08, 75.98, 70.36, 77.12, 73.24, 78.94, 70.86, 76.3, 76.86, 70.9, 80.74, 82.04, 73.0, 75.5, 69.9, 79.66, 75.7, 73.98, 72.12, 75.36, 76.38, 76.96]\n",
      "total accuracies train\n",
      "[84.065, 84.375, 88.335, 90.78, 90.065, 92.985, 92.83, 93.575, 92.615, 94.425, 94.7, 95.095, 94.89, 94.245, 94.985, 95.775, 93.795, 95.56, 96.385, 95.825, 94.635, 96.585, 96.215, 96.965, 96.52, 96.88, 97.165, 97.27, 96.86, 94.865, 97.51, 97.52, 97.15, 97.525, 96.475, 98.1, 97.855, 97.775, 97.325, 97.83, 97.05, 97.925, 97.83, 97.575, 97.26, 98.24, 98.62, 97.74, 97.77, 97.315, 98.235, 98.525, 97.36, 97.73, 98.745, 98.61, 97.735, 98.065, 98.585, 98.615]\n",
      "learning rate is\n",
      "0.1\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 69.3, Train Acc: 84.52\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 59.7, Train Acc: 85.285\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 82.56, Train Acc: 88.3\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 75.16, Train Acc: 90.365\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 75.56, Train Acc: 90.72\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 88.22, Train Acc: 90.04\n",
      "Average accuracy is75.0833333333\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 75.56, Train Acc: 91.045\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 64.18, Train Acc: 90.065\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 77.9, Train Acc: 91.83\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 85.2, Train Acc: 91.825\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 75.96, Train Acc: 91.155\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 75.68, Train Acc: 90.4\n",
      "Average accuracy is75.7466666667\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 79.98, Train Acc: 93.8\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 77.22, Train Acc: 93.645\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 72.06, Train Acc: 92.785\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 75.7, Train Acc: 91.865\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 78.92, Train Acc: 92.335\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 69.6, Train Acc: 90.665\n",
      "Average accuracy is75.58\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 77.28, Train Acc: 94.845\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 73.4, Train Acc: 94.19\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 74.72, Train Acc: 94.32\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 86.54, Train Acc: 91.615\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 72.52, Train Acc: 94.175\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 76.2, Train Acc: 94.025\n",
      "Average accuracy is76.7766666667\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 76.02, Train Acc: 95.44\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 80.76, Train Acc: 94.315\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 82.04, Train Acc: 93.58\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 82.22, Train Acc: 93.625\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 83.3, Train Acc: 93.625\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 77.68, Train Acc: 95.27\n",
      "Average accuracy is80.3366666667\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 73.48, Train Acc: 95.06\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 69.54, Train Acc: 94.07\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 79.06, Train Acc: 95.405\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 77.2, Train Acc: 95.175\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 86.3, Train Acc: 92.155\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 69.26, Train Acc: 94.295\n",
      "Average accuracy is75.8066666667\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 78.1, Train Acc: 96.8\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 76.9, Train Acc: 96.565\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 74.58, Train Acc: 95.86\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 78.08, Train Acc: 96.715\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 81.88, Train Acc: 93.865\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 77.3, Train Acc: 94.935\n",
      "Average accuracy is77.8066666667\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 76.26, Train Acc: 96.615\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 73.3, Train Acc: 96.585\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 80.66, Train Acc: 95.63\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 76.86, Train Acc: 94.065\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 75.72, Train Acc: 96.29\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 71.58, Train Acc: 95.275\n",
      "Average accuracy is75.73\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 77.94, Train Acc: 97.175\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 76.6, Train Acc: 96.825\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 84.62, Train Acc: 93.3\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 80.34, Train Acc: 95.695\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 69.22, Train Acc: 95.615\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 75.16, Train Acc: 96.705\n",
      "Average accuracy is77.3133333333\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 73.98, Train Acc: 97.4\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 79.24, Train Acc: 96.7\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 82.4, Train Acc: 95.24\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 76.24, Train Acc: 96.78\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 74.88, Train Acc: 97.67\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 76.54, Train Acc: 97.92\n",
      "Average accuracy is77.2133333333\n",
      "total average accuarcies validation\n",
      "[69.3, 59.7, 82.56, 75.16, 75.56, 88.22, 75.56, 64.18, 77.9, 85.2, 75.96, 75.68, 79.98, 77.22, 72.06, 75.7, 78.92, 69.6, 77.28, 73.4, 74.72, 86.54, 72.52, 76.2, 76.02, 80.76, 82.04, 82.22, 83.3, 77.68, 73.48, 69.54, 79.06, 77.2, 86.3, 69.26, 78.1, 76.9, 74.58, 78.08, 81.88, 77.3, 76.26, 73.3, 80.66, 76.86, 75.72, 71.58, 77.94, 76.6, 84.62, 80.34, 69.22, 75.16, 73.98, 79.24, 82.4, 76.24, 74.88, 76.54]\n",
      "total accuracies train\n",
      "[84.52, 85.285, 88.3, 90.365, 90.72, 90.04, 91.045, 90.065, 91.83, 91.825, 91.155, 90.4, 93.8, 93.645, 92.785, 91.865, 92.335, 90.665, 94.845, 94.19, 94.32, 91.615, 94.175, 94.025, 95.44, 94.315, 93.58, 93.625, 93.625, 95.27, 95.06, 94.07, 95.405, 95.175, 92.155, 94.295, 96.8, 96.565, 95.86, 96.715, 93.865, 94.935, 96.615, 96.585, 95.63, 94.065, 96.29, 95.275, 97.175, 96.825, 93.3, 95.695, 95.615, 96.705, 97.4, 96.7, 95.24, 96.78, 97.67, 97.92]\n",
      "learning rate is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 77.3, Train Acc: 84.445\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 58.52, Train Acc: 84.375\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 79.76, Train Acc: 87.505\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 66.7, Train Acc: 84.095\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 66.96, Train Acc: 88.11\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 78.64, Train Acc: 89.55\n",
      "Average accuracy is71.3133333333\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 74.9, Train Acc: 88.765\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 77.7, Train Acc: 89.255\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 73.24, Train Acc: 90.11\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 85.86, Train Acc: 87.095\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 81.68, Train Acc: 91.105\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 63.32, Train Acc: 86.54\n",
      "Average accuracy is76.1166666667\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 64.5, Train Acc: 89.915\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 72.88, Train Acc: 90.685\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 68.6, Train Acc: 90.53\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 69.38, Train Acc: 91.03\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 89.26, Train Acc: 84.82\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 64.92, Train Acc: 90.015\n",
      "Average accuracy is71.59\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 63.9, Train Acc: 90.605\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 71.56, Train Acc: 90.7\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 82.1, Train Acc: 92.295\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 80.62, Train Acc: 92.98\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 82.34, Train Acc: 92.635\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 74.42, Train Acc: 91.18\n",
      "Average accuracy is75.8233333333\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 75.66, Train Acc: 94.5\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 83.78, Train Acc: 91.765\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 78.82, Train Acc: 93.105\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 80.32, Train Acc: 93.49\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 73.54, Train Acc: 91.72\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 81.28, Train Acc: 93.15\n",
      "Average accuracy is78.9\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 68.0, Train Acc: 92.29\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 83.16, Train Acc: 91.135\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 67.66, Train Acc: 92.36\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 68.76, Train Acc: 93.36\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 75.26, Train Acc: 93.43\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 68.88, Train Acc: 91.92\n",
      "Average accuracy is71.9533333333\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 77.94, Train Acc: 93.01\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 74.26, Train Acc: 94.24\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 76.54, Train Acc: 94.31\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 69.84, Train Acc: 91.935\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 71.92, Train Acc: 94.39\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 73.78, Train Acc: 95.17\n",
      "Average accuracy is74.0466666667\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 73.28, Train Acc: 95.87\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 75.4, Train Acc: 94.835\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 79.88, Train Acc: 95.07\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 70.46, Train Acc: 95.045\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 74.56, Train Acc: 95.11\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 76.48, Train Acc: 94.91\n",
      "Average accuracy is75.01\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 81.98, Train Acc: 95.12\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 66.36, Train Acc: 93.76\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 81.46, Train Acc: 94.24\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 67.64, Train Acc: 92.57\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 82.24, Train Acc: 94.4\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 73.28, Train Acc: 95.985\n",
      "Average accuracy is75.4933333333\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 72.28, Train Acc: 96.08\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 63.52, Train Acc: 92.685\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 74.46, Train Acc: 95.53\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 78.04, Train Acc: 96.325\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 72.38, Train Acc: 95.555\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 79.76, Train Acc: 95.865\n",
      "Average accuracy is73.4066666667\n",
      "total average accuarcies validation\n",
      "[77.3, 58.52, 79.76, 66.7, 66.96, 78.64, 74.9, 77.7, 73.24, 85.86, 81.68, 63.32, 64.5, 72.88, 68.6, 69.38, 89.26, 64.92, 63.9, 71.56, 82.1, 80.62, 82.34, 74.42, 75.66, 83.78, 78.82, 80.32, 73.54, 81.28, 68.0, 83.16, 67.66, 68.76, 75.26, 68.88, 77.94, 74.26, 76.54, 69.84, 71.92, 73.78, 73.28, 75.4, 79.88, 70.46, 74.56, 76.48, 81.98, 66.36, 81.46, 67.64, 82.24, 73.28, 72.28, 63.52, 74.46, 78.04, 72.38, 79.76]\n",
      "total accuracies train\n",
      "[84.445, 84.375, 87.505, 84.095, 88.11, 89.55, 88.765, 89.255, 90.11, 87.095, 91.105, 86.54, 89.915, 90.685, 90.53, 91.03, 84.82, 90.015, 90.605, 90.7, 92.295, 92.98, 92.635, 91.18, 94.5, 91.765, 93.105, 93.49, 91.72, 93.15, 92.29, 91.135, 92.36, 93.36, 93.43, 91.92, 93.01, 94.24, 94.31, 91.935, 94.39, 95.17, 95.87, 94.835, 95.07, 95.045, 95.11, 94.91, 95.12, 93.76, 94.24, 92.57, 94.4, 95.985, 96.08, 92.685, 95.53, 96.325, 95.555, 95.865]\n",
      "learning rate is\n",
      "0.5\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 65.46, Train Acc: 80.605\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 68.9, Train Acc: 80.97\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 82.22, Train Acc: 82.14\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 68.02, Train Acc: 83.635\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 48.94, Train Acc: 80.425\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 65.28, Train Acc: 85.34\n",
      "Average accuracy is66.47\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 61.04, Train Acc: 86.21\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 69.02, Train Acc: 86.235\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 61.88, Train Acc: 85.39\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 74.4, Train Acc: 88.45\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 65.22, Train Acc: 83.055\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 66.3, Train Acc: 84.88\n",
      "Average accuracy is66.31\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 64.2, Train Acc: 87.985\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 77.4, Train Acc: 89.53\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 67.22, Train Acc: 86.975\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 68.14, Train Acc: 87.14\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 72.14, Train Acc: 88.66\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 66.52, Train Acc: 87.245\n",
      "Average accuracy is69.27\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 68.64, Train Acc: 91.325\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 72.16, Train Acc: 87.91\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 66.42, Train Acc: 89.01\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 81.0, Train Acc: 89.23\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 77.2, Train Acc: 90.94\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 83.58, Train Acc: 90.2\n",
      "Average accuracy is74.8333333333\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 85.96, Train Acc: 91.43\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 81.68, Train Acc: 92.495\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 78.26, Train Acc: 93.2\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 89.1, Train Acc: 86.28\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 81.34, Train Acc: 91.905\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 63.88, Train Acc: 90.37\n",
      "Average accuracy is80.0366666667\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 83.28, Train Acc: 92.67\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 80.48, Train Acc: 93.195\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 78.82, Train Acc: 94.14\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 76.22, Train Acc: 93.23\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 69.3, Train Acc: 93.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/10], Step: [601/625], Validation Acc: 82.1, Train Acc: 93.17\n",
      "Average accuracy is78.3666666667\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 78.5, Train Acc: 94.53\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 79.84, Train Acc: 92.93\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 77.72, Train Acc: 92.92\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 74.22, Train Acc: 94.49\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 81.58, Train Acc: 94.055\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 68.38, Train Acc: 92.16\n",
      "Average accuracy is76.7066666667\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 84.56, Train Acc: 91.82\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 73.62, Train Acc: 95.445\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 61.52, Train Acc: 91.35\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 80.72, Train Acc: 93.99\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 80.42, Train Acc: 94.08\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 71.18, Train Acc: 95.015\n",
      "Average accuracy is75.3366666667\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 79.62, Train Acc: 95.47\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 74.0, Train Acc: 95.345\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 81.7, Train Acc: 94.34\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 83.24, Train Acc: 94.015\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 57.56, Train Acc: 89.025\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 66.8, Train Acc: 93.41\n",
      "Average accuracy is73.82\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 85.2, Train Acc: 93.165\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 77.94, Train Acc: 95.63\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 72.42, Train Acc: 95.535\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 78.5, Train Acc: 96.005\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 69.0, Train Acc: 94.92\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 85.8, Train Acc: 92.335\n",
      "Average accuracy is78.1433333333\n",
      "total average accuarcies validation\n",
      "[65.46, 68.9, 82.22, 68.02, 48.94, 65.28, 61.04, 69.02, 61.88, 74.4, 65.22, 66.3, 64.2, 77.4, 67.22, 68.14, 72.14, 66.52, 68.64, 72.16, 66.42, 81.0, 77.2, 83.58, 85.96, 81.68, 78.26, 89.1, 81.34, 63.88, 83.28, 80.48, 78.82, 76.22, 69.3, 82.1, 78.5, 79.84, 77.72, 74.22, 81.58, 68.38, 84.56, 73.62, 61.52, 80.72, 80.42, 71.18, 79.62, 74.0, 81.7, 83.24, 57.56, 66.8, 85.2, 77.94, 72.42, 78.5, 69.0, 85.8]\n",
      "total accuracies train\n",
      "[80.605, 80.97, 82.14, 83.635, 80.425, 85.34, 86.21, 86.235, 85.39, 88.45, 83.055, 84.88, 87.985, 89.53, 86.975, 87.14, 88.66, 87.245, 91.325, 87.91, 89.01, 89.23, 90.94, 90.2, 91.43, 92.495, 93.2, 86.28, 91.905, 90.37, 92.67, 93.195, 94.14, 93.23, 93.04, 93.17, 94.53, 92.93, 92.92, 94.49, 94.055, 92.16, 91.82, 95.445, 91.35, 93.99, 94.08, 95.015, 95.47, 95.345, 94.34, 94.015, 89.025, 93.41, 93.165, 95.63, 95.535, 96.005, 94.92, 92.335]\n",
      "learning rate is\n",
      "1\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 81.62, Train Acc: 80.935\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 90.54, Train Acc: 74.3\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 67.8, Train Acc: 78.675\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 51.66, Train Acc: 82.315\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 82.54, Train Acc: 79.685\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 67.14, Train Acc: 85.96\n",
      "Average accuracy is73.55\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 65.38, Train Acc: 86.12\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 75.8, Train Acc: 83.995\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 84.44, Train Acc: 80.22\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 78.7, Train Acc: 86.355\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 50.96, Train Acc: 83.825\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 66.36, Train Acc: 87.08\n",
      "Average accuracy is70.2733333333\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 68.14, Train Acc: 89.8\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 55.56, Train Acc: 86.33\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 75.7, Train Acc: 89.445\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 70.5, Train Acc: 86.255\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 80.24, Train Acc: 87.38\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 62.8, Train Acc: 88.205\n",
      "Average accuracy is68.8233333333\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 61.02, Train Acc: 87.635\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 64.02, Train Acc: 89.125\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 78.12, Train Acc: 90.3\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 79.72, Train Acc: 90.635\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 84.76, Train Acc: 89.005\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 66.6, Train Acc: 89.055\n",
      "Average accuracy is72.3733333333\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 68.84, Train Acc: 92.0\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 76.18, Train Acc: 92.445\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 68.52, Train Acc: 89.56\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 71.56, Train Acc: 88.525\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 65.88, Train Acc: 90.39\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 77.1, Train Acc: 92.89\n",
      "Average accuracy is71.3466666667\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 80.1, Train Acc: 93.34\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 87.38, Train Acc: 90.27\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 76.22, Train Acc: 92.46\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 83.94, Train Acc: 92.255\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 83.16, Train Acc: 92.05\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 76.24, Train Acc: 94.04\n",
      "Average accuracy is81.1733333333\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 74.32, Train Acc: 94.335\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 80.78, Train Acc: 93.02\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 82.48, Train Acc: 93.245\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 79.14, Train Acc: 93.84\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 82.1, Train Acc: 92.395\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 85.32, Train Acc: 90.345\n",
      "Average accuracy is80.69\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 79.8, Train Acc: 94.225\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 78.4, Train Acc: 94.09\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 79.44, Train Acc: 94.81\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 85.16, Train Acc: 92.56\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 74.64, Train Acc: 94.83\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 81.06, Train Acc: 93.695\n",
      "Average accuracy is79.75\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 86.02, Train Acc: 92.26\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 71.46, Train Acc: 94.97\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 66.64, Train Acc: 92.73\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 74.46, Train Acc: 94.63\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 73.22, Train Acc: 94.79\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 77.7, Train Acc: 95.115\n",
      "Average accuracy is74.9166666667\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 76.02, Train Acc: 96.04\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 85.5, Train Acc: 91.825\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 79.24, Train Acc: 95.705\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 75.34, Train Acc: 95.54\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 72.28, Train Acc: 95.13\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 76.72, Train Acc: 95.285\n",
      "Average accuracy is77.5166666667\n",
      "total average accuarcies validation\n",
      "[81.62, 90.54, 67.8, 51.66, 82.54, 67.14, 65.38, 75.8, 84.44, 78.7, 50.96, 66.36, 68.14, 55.56, 75.7, 70.5, 80.24, 62.8, 61.02, 64.02, 78.12, 79.72, 84.76, 66.6, 68.84, 76.18, 68.52, 71.56, 65.88, 77.1, 80.1, 87.38, 76.22, 83.94, 83.16, 76.24, 74.32, 80.78, 82.48, 79.14, 82.1, 85.32, 79.8, 78.4, 79.44, 85.16, 74.64, 81.06, 86.02, 71.46, 66.64, 74.46, 73.22, 77.7, 76.02, 85.5, 79.24, 75.34, 72.28, 76.72]\n",
      "total accuracies train\n",
      "[80.935, 74.3, 78.675, 82.315, 79.685, 85.96, 86.12, 83.995, 80.22, 86.355, 83.825, 87.08, 89.8, 86.33, 89.445, 86.255, 87.38, 88.205, 87.635, 89.125, 90.3, 90.635, 89.005, 89.055, 92.0, 92.445, 89.56, 88.525, 90.39, 92.89, 93.34, 90.27, 92.46, 92.255, 92.05, 94.04, 94.335, 93.02, 93.245, 93.84, 92.395, 90.345, 94.225, 94.09, 94.81, 92.56, 94.83, 93.695, 92.26, 94.97, 92.73, 94.63, 94.79, 95.115, 96.04, 91.825, 95.705, 95.54, 95.13, 95.285]\n",
      "learning rate is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 71.38, Train Acc: 80.035\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 55.14, Train Acc: 77.38\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 51.48, Train Acc: 77.105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0cbfbdcdd281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                                                \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmoviegroup_collate_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                                shuffle=False)\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtest_model_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-ac437856efe2>\u001b[0m in \u001b[0;36mtest_model_routine\u001b[0;34m(train_loader, val_loader, model, criterion, optimizer, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizing based on learning rate \n",
    "\n",
    "for learning_rate in [0.02, 0.05, 0.1, 0.2, 0.5, 1, 2]:\n",
    "    print(\"learning rate is\")\n",
    "    print(str(learning_rate))\n",
    "    token2id, id2token = build_vocab(unigram_flattened_traintokens, 10000)\n",
    "    new_model = BagOfWords(len(id2token), 100)\n",
    "    optimizer = torch.optim.Adam(new_model.parameters(), lr = learning_rate)\n",
    "    train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "    val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "    test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "    BATCH_SIZE = 32\n",
    "    train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "    val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=False)\n",
    "    test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 84.3, Train Acc: 41.76\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 84.3, Train Acc: 41.76\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 84.3, Train Acc: 41.76\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 84.3, Train Acc: 41.76\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 84.3, Train Acc: 41.76\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 84.3, Train Acc: 41.76\n",
      "Average accuracy is84.3\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 84.3, Train Acc: 41.76\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 84.3, Train Acc: 41.76\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 84.3, Train Acc: 41.76\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 84.3, Train Acc: 41.76\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 84.3, Train Acc: 41.76\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-5404ff45821f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                                            shuffle=False)\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtest_model_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-85a4f0189c95>\u001b[0m in \u001b[0;36mtest_model_routine\u001b[0;34m(train_loader, val_loader, model, criterion, optimizer, num_epochs, learning_rate, scheduler)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now, try with linear annealing. \n",
    "token2id, id2token = build_vocab(unigram_flattened_traintokens, 10000)\n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "decay=0.01/num_epochs\n",
    "optimizer = torch.optim.Adam(new_model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Average accuracy is0.0\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Average accuracy is0.0\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Average accuracy is0.0\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Average accuracy is0.0\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 100.0, Train Acc: 37.5\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Average accuracy is16.6666666667\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Average accuracy is0.0\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Average accuracy is0.0\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Average accuracy is0.0\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Average accuracy is0.0\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 0.0, Train Acc: 62.5\n",
      "Average accuracy is0.0\n",
      "total average accuarcies validation\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "total accuracies train\n",
      "[62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 37.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  100.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  37.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5,\n",
       "  62.5])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Try out Adam decay \n",
    "token2id, id2token = build_vocab(unigram_flattened_traintokens, 10000)\n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "optimizer = torch.optim.Adam(new_model.parameters(), lr = 0.1, weight_decay=True)\n",
    "train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Annealing | Maximum Validation Accuracy |\n",
    "|---------------|-----------------------------|\n",
    "| No annealing     | 85.98                      |\n",
    "| With annealing        | 85.24                    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/625], Validation Acc: 44.58, Train Acc: 77.295\n",
      "Epoch: [1/3], Step: [201/625], Validation Acc: 41.68, Train Acc: 77.04\n",
      "Epoch: [1/3], Step: [301/625], Validation Acc: 61.74, Train Acc: 82.07\n",
      "Epoch: [1/3], Step: [401/625], Validation Acc: 81.98, Train Acc: 78.945\n",
      "Epoch: [1/3], Step: [501/625], Validation Acc: 67.66, Train Acc: 82.99\n",
      "Epoch: [1/3], Step: [601/625], Validation Acc: 58.66, Train Acc: 81.385\n",
      "Average accuracy is59.3833333333\n",
      "Epoch: [2/3], Step: [101/625], Validation Acc: 75.9, Train Acc: 82.945\n",
      "Epoch: [2/3], Step: [201/625], Validation Acc: 82.74, Train Acc: 79.92\n",
      "Epoch: [2/3], Step: [301/625], Validation Acc: 57.02, Train Acc: 81.825\n",
      "Epoch: [2/3], Step: [401/625], Validation Acc: 68.58, Train Acc: 83.6\n",
      "Epoch: [2/3], Step: [501/625], Validation Acc: 56.24, Train Acc: 81.73\n",
      "Epoch: [2/3], Step: [601/625], Validation Acc: 75.24, Train Acc: 82.335\n",
      "Average accuracy is69.2866666667\n",
      "Epoch: [3/3], Step: [101/625], Validation Acc: 68.1, Train Acc: 83.6\n",
      "Epoch: [3/3], Step: [201/625], Validation Acc: 92.54, Train Acc: 72.915\n",
      "Epoch: [3/3], Step: [301/625], Validation Acc: 47.92, Train Acc: 77.62\n",
      "Epoch: [3/3], Step: [401/625], Validation Acc: 54.42, Train Acc: 80.135\n",
      "Epoch: [3/3], Step: [501/625], Validation Acc: 69.18, Train Acc: 81.265\n",
      "Epoch: [3/3], Step: [601/625], Validation Acc: 45.98, Train Acc: 77.845\n",
      "Average accuracy is63.0233333333\n",
      "total average accuarcies validation\n",
      "[44.58, 41.68, 61.74, 81.98, 67.66, 58.66, 75.9, 82.74, 57.02, 68.58, 56.24, 75.24, 68.1, 92.54, 47.92, 54.42, 69.18, 45.98]\n",
      "total accuracies train\n",
      "[77.295, 77.04, 82.07, 78.945, 82.99, 81.385, 82.945, 79.92, 81.825, 83.6, 81.73, 82.335, 83.6, 72.915, 77.62, 80.135, 81.265, 77.845]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78.128"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id, id2token = build_vocab(uigram_flattened_traintokens, 100000)\n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "optimizer = torch.optim.Adam(new_model.parameters(), lr = 0.1)\n",
    "train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "acc1, acc2, model = test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n",
    "\"\"\"\n",
    "Testing the model\n",
    "\"\"\"\n",
    "test_model(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHelp function that tests the model's performance on a dataset\\n@param: loader - data loader for the dataset to test against\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_raw, test_labels_raw = get_test_train(\"test\")   \n",
    "\"\"\"\n",
    "Help function that tests the model's performance on a dataset\n",
    "@param: loader - data loader for the dataset to test against\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is 0\n",
      "and model got it wrong\n",
      "Index is 1\n",
      "and model got it right\n",
      "Index is 2\n",
      "and model got it right\n",
      "Index is 3\n",
      "and model got it right\n",
      "Index is 4\n",
      "and model got it right\n",
      "Index is 5\n",
      "and model got it right\n",
      "Index is 6\n",
      "and model got it right\n",
      "Index is 7\n",
      "and model got it right\n",
      "Index is 8\n",
      "and model got it right\n",
      "Index is 9\n",
      "and model got it wrong\n",
      "Index is 10\n",
      "and model got it right\n",
      "Index is 11\n",
      "and model got it right\n",
      "Index is 12\n",
      "and model got it right\n",
      "Index is 13\n",
      "and model got it right\n",
      "Index is 14\n",
      "and model got it right\n",
      "Index is 15\n",
      "and model got it wrong\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "curr = 0\n",
    "three_example_right = []\n",
    "three_example_wrong = []\n",
    "\n",
    "for data, lengths, labels in val_loader:\n",
    "    if len(three_example_right) < 3 or len(three_example_wrong) < 3:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        if predicted != labels:\n",
    "            print(\"Index is \"+str(curr))\n",
    "            print(\"and model got it wrong\")\n",
    "            three_example_wrong.append(curr)\n",
    "        else:\n",
    "            print(\"Index is \"+str(curr))\n",
    "            print(\"and model got it right\") \n",
    "            three_example_right.append(curr)\n",
    "        curr += 1\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = get_test_train(\"train\")\n",
    "LEN_TRAIN = len(train_data)\n",
    "\n",
    "val_data = train_data[int(round(LEN_TRAIN*0.8)):]\n",
    "val_labels = train_labels[int(round(LEN_TRAIN*0.8)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model got these three examples wrong \n",
      "\n",
      "I would have liked to give this movie a zero but that wasn't an option!! This movie sucks!!! The women cannot act. i should have known it was gonna suck when i saw Bobby Brown. Nobody in my house could believe i hadn't changed the channel after the first 15 minutes. the idea of black females as gunslingers in the western days is ridiculous. it's not just a race thing, it's also a gender. the combination of the two things is ridiculous.i am sorry because some of the people in the movie aren't bad actors/actresses but the movie itself was awful. it was not credible as a movie. it might be 'entertaining' to a certain group of people but i am not in that group. lol. and using a great line from a great, great movie...\"that's all I have to say about that.\"\n",
      "\n",
      "\n",
      "I am probably one of the few viewers who would not recommend this film. Thought visually stunning like all of Ang Lee's work (each still frame seems worthy of a print), I was really disappointed by the film's disjointed pace. It really was too long.<br /><br />The story is set in Civil War era Missouri, and is about a young man (Roedel) who joins the feral forces of the Bushwackers, sort of renegade Confederate sympathizers who conduct geurilla type fighting with the Jayhawkers, their Union counterparts. He and his close friend, Jack Bull Chiles played by Skeet Ulrich, join the group after Chile's father is shot point-blank and his home is burned, presumably by Jayhawkers. The story follows Roedel's and Chiles' raiding adventures and their interactions with other victims of the war, including former slave who fights for the Bushwhackers (Daniel Holt played by Jeffery Wright), and a war widow played by Jewel.<br /><br />It seemed that every time the film developed the story to an interesting point, it would turn to some other subplot and leave things undeveloped. For example, the agitation among Roedel's group caused by former slave Holt participating in the confederate cause is shown briefly through some conflict regarding propriety and protocol, and then dropped until later in the movie. A young villian/bully Bushwhacker hates Roedel and directs much angst and violence against him, but, we never know why. Some of the characters never seem to surface; I think that is because the movie embraces too many of them as well as taking on large amounts of history.<br /><br />The historical detail was excellent. I loved looking at the housing, furniture, clothes, etc., and I thought the lead actors did a wonderful job of humanizing the characters, though they stumbled a bit with the dialog. Unless you really enjoy history or are a huge Ang Lee fan, though, take a pass on this one.\n",
      "\n",
      "\n",
      "MONSTER - He was great; I loved the special-effects which created this monster which looked like an updated version of \"The Creature From the Black Lagoon.\" The scenes with this beast roaming the land and capturing people ranged from good to jaw-dropping.<br /><br />SOCIAL COMMENTARY: Much of the story takes place in the quarantine area as the doctors (under orders from the government) state that SARS-like disease is out there. In a nutshell, we get the familiar government cover-up story. You know, I expect this Liberal paranoid mindset with Hollywood films always painting our government as corrupt, but it looks like the Koreans are copying the format, and it's very tedious. In here, it takes away from the excitement and suspense of this \"monster.\" It just drags the film down. The main family featured in the film has to watch from a distance while the young girl in their family, presumed dead, was hauled off by the creature.<br /><br />MORAL: A typical \"don't pollute the water\" message because this is what can happen - a horrible mutated monster. This used to be the anti-nuclear bomb message from the 1950 when radiation caused giants ants, spiders, fish or whatever in those schlocky sci-fi films. Now its \"environmental issues\" that are the focus.<br /><br />THE HUMOR This was mostly stupid. I normally laugh at slapstick but this wasn't funny. I don't know if the Korean sense of humor is that pitiful, or the film was purposely trying to be ultra-corny with a take on the old \"Godzilla\" movies. Let's hope it's the latter.<br /><br />TRANSFER - The video transfer was good. This was a sharp-looking picture and sound was decent with a lot rear-speaker crowd noise. I watched this in Korean with the English subtitles. That might have been a mistake as the Korean guttural voice sounds got annoying after a half hour.<br /><br />OVERALL - This had a promise but turned out to be a big letdown and even boring in too many spots, which is inexcusable for a modern-day monster film. Two hours was WAY too long for this story. How this film drew record crowds in Korea I don't know. They must not have much in the way of films to enjoy and support.\n",
      "\n",
      "\n",
      "model got these three examples right \n",
      "\n",
      "It's a waist to indulge such great actors in such a weak and boring movie. Besides all the unanswered questions posted in the other comments, what's so difficult about capturing the robbers? Just eliminate the bank workers, see who was at the bank-from all the cameras' footage angles-prior to the robbers entry and you have those extra 4 remaining robbers among the hostages. Where is the suspense every body is talking about? It was so obvious the moment the hostages were asked to change into this identical uniform that they were all going to walk out the front door... seen it many times. At least Mr. Spike Lee could have seasoned the movie with some good music score and artistic shooting. The Movie is not worth it. Pronto!\n",
      "\n",
      "\n",
      "Unusual? Yes!<br /><br />Unusual setting for an American wartime movie, New Zealand.<br /><br />Unusual subject matter, four sisters and their relationships with American soldiers, from one bearing the illegitimate child of the dead son of a Senator, to another living with seven Marines (one at a time) before being murdered by her returning POW New Zealander husband.<br /><br />Unusual to see Paul Newman deliver such a poor performance so soon after his unforgettable role as Rocky Graziano in the brilliant \"Somebody Up There Loves Me\".<br /><br />Unusual for two fine \"Stars\" Joan Fontaine and Jean Simmons, to leave so little of themselves on a movie.<br /><br />Unusual that I could be bothered to write a review of such a poor film, give it a miss!\n",
      "\n",
      "\n",
      "This movie was so bad, outdated and stupid that I had rough times to watch it to the end. I had seen this Rodney guy in Natural Born Killers and I thought he was funny as hell in it, but this movie was crap. The \"jokes\" weren't funny, actors weren't funny, anything about it wasn't even remotely funny. Don't waste your time for this! Only positive things about this were the beautiful wives :) and Molly Shannon who I'm sure tried her best, but the script was just too awful. That's why I rated it \"2\" instead of \"1\", but it's definitely one of the worst films I've ever seen.\n"
     ]
    }
   ],
   "source": [
    "print(\"model got these three examples wrong \\n\")\n",
    "print(val_data[0])\n",
    "print(\"\\n\")\n",
    "print(val_data[9])\n",
    "print(\"\\n\")\n",
    "print(val_data[15])\n",
    "print(\"\\n\")\n",
    "print(\"model got these three examples right \\n\")\n",
    "print(val_data[1])\n",
    "print(\"\\n\")\n",
    "print(val_data[2])\n",
    "print(\"\\n\")\n",
    "print(val_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try varying the batch size! \n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "optimizer = torch.optim.Adam(new_model.parameters(), lr = 0.1)\n",
    "train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "BATCH_SIZE = 128\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "acc1, acc2, model = test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
