{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "# split train data into actual train and validation set. \n",
    "DIRLINK = \"aclImdb\"\n",
    "def random_shuffle(threads, labels):\n",
    "\t\"\"\"\n",
    "\tthis function randomly shuffles the data and labels\n",
    "\t\"\"\"\n",
    "\tthreads = np.array(threads)\n",
    "\tlabels = np.array(labels)\n",
    "\ts = np.arange(labels.shape[0])\n",
    "\tnp.random.shuffle(s)\n",
    "\tthreads = threads[s]\n",
    "\tlabels = labels[s]\n",
    "\treturn threads, labels\n",
    "\n",
    "def get_test_train(traintest):\n",
    "    positive = [open(DIRLINK+\"/\"+traintest+\"/pos/\"+file).read() for file in os.listdir(DIRLINK+\"/\"+traintest+\"/pos/\")]\n",
    "    negative = [open(DIRLINK+\"/\"+traintest+\"/neg/\"+file).read() for file in os.listdir(DIRLINK+\"/\"+traintest+\"/neg/\")]\n",
    "    labels = [1]* len(positive)\n",
    "    data = positive\n",
    "    data.extend(negative)\n",
    "    labels.extend([0]*len(negative))\n",
    "    # shuffle the dataset\n",
    "    return data, labels \n",
    "\n",
    "#train_data, train_labels = get_test_train(\"train\")\n",
    "#test_data, test_labels = get_test_train(\"test\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Split train data into actual train and validation sets\n",
    "LEN_TRAIN = len(train_data)\n",
    "LEN_TEST = len(test_data)\n",
    "train_datadf = train_data[:int(round(LEN_TRAIN*0.8))]\n",
    "train_labeldf = train_labels[:int(round(LEN_TRAIN*0.8))]\n",
    "val_data = train_data[int(round(LEN_TRAIN*0.8)):]\n",
    "val_labels = train_labels[int(round(LEN_TRAIN*0.8)):]\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_datadf)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Random sample from train dataset\n",
    "import random\n",
    "print (train_data[random.randint(0, len(train_data) - 1)])\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to do with Tokenization\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import spacy\n",
    "import string\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "!pip install nltk\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [str(token).lower() for token in parsed if (str(token) not in punctuations)]\n",
    "\n",
    "def get_ngram(sample, n_gram):\n",
    "    sample = sample.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(sample)-n_gram+1):\n",
    "        output.append(sample[i:i+n_gram])\n",
    "    final = [' '.join(x) for x in output]\n",
    "    return final\n",
    "\n",
    "def strip_html(sample): \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sample)\n",
    "    return cleantext \n",
    "\n",
    "def tokenize_dataset(dataset, name, n_gram):\n",
    "    token_dataset = {i:[] for i in range(1, n_gram + 1)}\n",
    "    all_datasets = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        sample = str(sample)\n",
    "        #sample = strip_html(sample)\n",
    "        for i in range(1, n_gram+1):\n",
    "            sample_after = get_ngram(sample, i)\n",
    "            tokens = lower_case_remove_punc(sample_after)\n",
    "            token_dataset[i].append(tokens)\n",
    "    for i in range(1, n_gram+1):\n",
    "        pickle.dump(token_dataset[i],open(name+\"tokens_dataset\"+str(i)+\".pkl\", \"wb\") ) \n",
    "        all_datasets.append(token_dataset[i])\n",
    "    return all_datasets\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(dataset):\n",
    "    new_dataset = []\n",
    "    for review in dataset:\n",
    "        new_review = []\n",
    "        for word in review:\n",
    "            new_review.append(wordnet_lemmatizer.lemmatize(word))\n",
    "        new_dataset.append(\"\".join(new_review))\n",
    "    return new_dataset\n",
    "\n",
    "#tokenized_train = tokenize_dataset(lemmatize(train_datadf), \"trainlemma\", 2)\n",
    "#tokenized_val = tokenize_dataset(lemmatize(val_data) , \"vallemma\", 2)\n",
    "#tokenized_test = tokenize_dataset(lemmatize(test_data), \"testlemma\", 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "unigram_train = pickle.load(open(\"trainlemmatokens_dataset2.pkl\", \"rb\"))\n",
    "unigram_val = pickle.load(open(\"vallemmatokens_dataset2.pkl\", \"rb\"))\n",
    "unigram_test = pickle.load(open(\"testlemmatokens_dataset2.pkl\", \"rb\"))\n",
    "train_labeldf = pickle.load(open(\"traintokens_labels.pkl\", \"rb\") )\n",
    "val_labels = pickle.load(open(\"valtokens_labels.pkl\", \"rb\") )\n",
    "test_labels = pickle.load(open(\"testtokens_labels.pkl\", \"rb\") )\n",
    "\n",
    "pickle.dump(train_labeldf,open(\"traintokens_labels.pkl\", \"wb\") ) \n",
    "pickle.dump(val_labels,open(\"valtokens_labels.pkl\", \"wb\") ) \n",
    "pickle.dump(test_labels,open(\"testtokens_labels.pkl\", \"wb\") ) \n",
    "# these are the labels\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "//I'm doing this to be able to move cells up or down easily. \n",
    "IPython.keyboard_manager.command_shortcuts.add_shortcut('Ctrl-k', {\n",
    "    help : 'move up selected cells',\n",
    "    help_index : 'jupyter-notebook:move-selection-up',\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.move_selection_up();\n",
    "        return false;\n",
    "    }}\n",
    ");\n",
    "\n",
    "IPython.keyboard_manager.command_shortcuts.add_shortcut('Ctrl-j', {\n",
    "    help : 'move down selected cells',\n",
    "    help_index : 'jupyter-notebook:move-selection-down',\n",
    "    handler :  function (event) {\n",
    "        IPython.notebook.move_selection_down();\n",
    "        return false;\n",
    "    }}\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# find the 1000 most common words and subwords. \n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "# flatten the toeknized train to become list of all the tokens\n",
    "#unigram_flattened_traintokens = [j for sub in unigram_train for j in sub]\n",
    "\n",
    "#zoken2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token2id, id2token = build_vocab(unigram_flattened_traintokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\"\"\"\n",
    "train_data_indices = token2index_dataset(unigram_train)\n",
    "val_data_indices = token2index_dataset(unigram_val)\n",
    "test_data_indices = token2index_dataset(unigram_test)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(len(train_labeldf))\n",
    "print(len(val_labels))\n",
    "print(len(test_labels))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "# change this based on whicehver n-gram file you're reading in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def moviegroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "\"\"\"\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        # view basically reshapes it, so this averages it out. \n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "#model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 3 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def test_model_routine(train_loader, val_loader, model, criterion, optimizer, num_epochs, learning_rate, scheduler=None):\n",
    "    acc_per_step_val = []\n",
    "    acc_per_step_train = []\n",
    "    for epoch in range(num_epochs):\n",
    "        acc = []\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model) \n",
    "                train_acc = test_model(train_loader, model)\n",
    "                acc.append(val_acc)\n",
    "                acc_per_step_val.append(val_acc)\n",
    "                acc_per_step_train.append(train_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Train Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc, train_acc))\n",
    "        #scheduler.step(loss)\n",
    "        print(\"Average accuracy is\"+ str(np.mean(acc)))\n",
    "    print(\"total average accuarcies validation\")\n",
    "    print(acc_per_step_val)\n",
    "    print(\"total accuracies train\")\n",
    "    print(acc_per_step_train)\n",
    "    return acc_per_step_val, acc_per_step_train, model\n",
    "#test_model_routine(train_loader, val_loader, model, criterion, optimizer, num_epochs, learning_rate)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NOW, LET'S USE ONLY UNIGRAM\n",
    "unigram_train = pickle.load(open(\"traintokens_dataset1.pkl\", \"rb\"))\n",
    "unigram_val = pickle.load(open(\"valtokens_dataset1.pkl\", \"rb\"))\n",
    "unigram_test = pickle.load(open(\"testtokens_dataset1.pkl\", \"rb\"))\n",
    "train_labeldf = pickle.load(open(\"traintokens_labels.pkl\", \"rb\") )\n",
    "val_labels = pickle.load(open(\"valtokens_labels.pkl\", \"rb\") )\n",
    "test_labels = pickle.load(open(\"testtokens_labels.pkl\", \"rb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [20000, 50000, 100000, 200000]\n",
    "for size in sizes:\n",
    "    print(\"Now varying vocab size\" + str(size))\n",
    "    unigram_flattened_traintokens = [j for sub in unigram_train for j in sub]\n",
    "    token2id, id2token = build_vocab(unigram_flattened_traintokens, size)\n",
    "    new_model = BagOfWords(len(id2token), emb_dim)\n",
    "    optimizer = torch.optim.Adam(new_model.parameters(), lr=learning_rate)\n",
    "    train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "    val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "    test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "    BATCH_SIZE = 32\n",
    "    train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "    val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=False)\n",
    "    test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now varying embedding size \n",
    "\n",
    "sizes = [200, 500, 1000, 5000]\n",
    "for size in sizes:\n",
    "    print(\"Now varying vocab size\" + str(size))\n",
    "    token2id, id2token = build_vocab(unigram_flattened_traintokens, 10000)\n",
    "    new_model = BagOfWords(len(id2token), size)\n",
    "    optimizer = torch.optim.Adam(new_model.parameters(), lr=learning_rate)\n",
    "    train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "    val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "    test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "    BATCH_SIZE = 32\n",
    "    train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "    val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=False)\n",
    "    test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying with new optimizer \n",
    "token2id, id2token = build_vocab(unigram_flattened_traintokens, 10000)\n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "optimizer = torch.optim.SGD(new_model.parameters(), lr = 0.1, momentum =0.05)\n",
    "train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing based on learning rate \n",
    "\n",
    "for learning_rate in [0.02, 0.05, 0.1, 0.2, 0.5, 1, 2]:\n",
    "    print(\"learning rate is\")\n",
    "    print(str(learning_rate))\n",
    "    token2id, id2token = build_vocab(unigram_flattened_traintokens, 10000)\n",
    "    new_model = BagOfWords(len(id2token), 100)\n",
    "    optimizer = torch.optim.Adam(new_model.parameters(), lr = learning_rate)\n",
    "    train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "    val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "    test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "    BATCH_SIZE = 32\n",
    "    train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "    val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=moviegroup_collate_func,\n",
    "                                               shuffle=False)\n",
    "    test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, try with linear annealing. \n",
    "token2id, id2token = build_vocab(unigram_flattened_traintokens, 10000)\n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "decay=0.01/num_epochs\n",
    "optimizer = torch.optim.Adam(new_model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try out Adam decay \n",
    "token2id, id2token = build_vocab(unigram_flattened_traintokens, 10000)\n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "optimizer = torch.optim.Adam(new_model.parameters(), lr = 0.1, weight_decay=True)\n",
    "train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Annealing | Maximum Validation Accuracy |\n",
    "|---------------|-----------------------------|\n",
    "| No annealing     | 85.98                      |\n",
    "| With annealing        | 85.24                    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token = build_vocab(uigram_flattened_traintokens, 100000)\n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "optimizer = torch.optim.Adam(new_model.parameters(), lr = 0.1)\n",
    "train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "acc1, acc2, model = test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n",
    "\"\"\"\n",
    "Testing the model\n",
    "\"\"\"\n",
    "test_model(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_raw, test_labels_raw = get_test_train(\"test\")   \n",
    "\"\"\"\n",
    "Help function that tests the model's performance on a dataset\n",
    "@param: loader - data loader for the dataset to test against\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "curr = 0\n",
    "three_example_right = []\n",
    "three_example_wrong = []\n",
    "\n",
    "for data, lengths, labels in val_loader:\n",
    "    if len(three_example_right) < 3 or len(three_example_wrong) < 3:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        if predicted != labels:\n",
    "            print(\"Index is \"+str(curr))\n",
    "            print(\"and model got it wrong\")\n",
    "            three_example_wrong.append(curr)\n",
    "        else:\n",
    "            print(\"Index is \"+str(curr))\n",
    "            print(\"and model got it right\") \n",
    "            three_example_right.append(curr)\n",
    "        curr += 1\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = get_test_train(\"train\")\n",
    "LEN_TRAIN = len(train_data)\n",
    "\n",
    "val_data = train_data[int(round(LEN_TRAIN*0.8)):]\n",
    "val_labels = train_labels[int(round(LEN_TRAIN*0.8)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model got these three examples wrong \\n\")\n",
    "print(val_data[0])\n",
    "print(\"\\n\")\n",
    "print(val_data[9])\n",
    "print(\"\\n\")\n",
    "print(val_data[15])\n",
    "print(\"\\n\")\n",
    "print(\"model got these three examples right \\n\")\n",
    "print(val_data[1])\n",
    "print(\"\\n\")\n",
    "print(val_data[2])\n",
    "print(\"\\n\")\n",
    "print(val_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try varying the batch size! \n",
    "new_model = BagOfWords(len(id2token), 100)\n",
    "optimizer = torch.optim.Adam(new_model.parameters(), lr = 0.1)\n",
    "train_data_indices = token2index_dataset(unigram_train, token2id)\n",
    "val_data_indices = token2index_dataset(unigram_val, token2id)\n",
    "test_data_indices = token2index_dataset(unigram_test,token2id)\n",
    "BATCH_SIZE = 128\n",
    "train_dataset = MovieGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = MovieGroupDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieGroupDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviegroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "acc1, acc2, model = test_model_routine(train_loader, val_loader, new_model, criterion, optimizer, num_epochs, learning_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
